{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOl5r2j5DN0DHfTwNQMB0VW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jevliu/2022-Machine-Learning-Specialization/blob/main/OPT_ESB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-NQ0iN2YWT2",
        "outputId": "3437b3ed-4473-46be-e89a-ae6852d122e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_ua7ZUk7YEl5"
      },
      "outputs": [],
      "source": [
        "from transformers import OPTForCausalLM, GPT2Tokenizer\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize_scalar\n",
        "import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def esb_quantize(weights, b, k, alpha, eps=1e-8):\n",
        "    # 计算N\n",
        "    N = 2 ** (b - k - 1) - 1\n",
        "\n",
        "    # 生成移位因子xi\n",
        "    xi = [2 ** -k] + [2 ** (i - k - 1) for i in range(1, N + 1)]\n",
        "\n",
        "    # 生成分数集合Omega\n",
        "    Omega = [[0] + list(range(1, 2 ** k))] + [list(range(2 ** k, 2 ** (k + 1))) for _ in range(N)]\n",
        "\n",
        "    # 生成非负的量化值集合Q_positive\n",
        "    Q_positive = []\n",
        "    for i in range(N + 1):\n",
        "        Q_positive.extend([alpha * x * y for x in xi[i:] for y in Omega[i]])\n",
        "    Q_positive = sorted(set(Q_positive))\n",
        "\n",
        "    # 对量化值集合进行对称处理\n",
        "    Q_e = sorted(set([-q for q in Q_positive] + [0] + Q_positive))\n",
        "\n",
        "    # 计算量化范围C\n",
        "    C = np.max(Q_e)\n",
        "\n",
        "    # 创建掩码,标记非零权重\n",
        "    mask = weights != 0\n",
        "\n",
        "    # 缩放非零权重\n",
        "    scaled_weights = np.zeros_like(weights)\n",
        "    scaled_weights[mask] = weights[mask] / alpha\n",
        "\n",
        "    # 截断非零权重\n",
        "    clipped_weights = np.clip(scaled_weights, -C, C)\n",
        "\n",
        "    # 移位量化非零权重\n",
        "    quantized_weights = np.zeros_like(clipped_weights)\n",
        "    nonzero_indices = np.where(mask)\n",
        "    v = clipped_weights[nonzero_indices]\n",
        "    n = np.ceil(np.log2(np.abs(v) + eps))\n",
        "    quantized_v = np.round(v / (2 ** (n - k))) * (2 ** (n - k))\n",
        "    quantized_weights[nonzero_indices] = quantized_v\n",
        "\n",
        "    # 反缩放量化后的权重\n",
        "    dequantized_weights = quantized_weights * alpha\n",
        "\n",
        "    return dequantized_weights"
      ],
      "metadata": {
        "id": "EQS4QfsnYdG9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = random_array = np.random.uniform(-1, 1, 10)\n",
        "# weights = c_attn_weights\n",
        "\n",
        "# 量化参数\n",
        "bits = 4\n",
        "k = 0\n",
        "alpha = alpha = np.max(np.abs(weights)) / (2**(bits-1)-1)\n",
        "\n",
        "# 量化\n",
        "quantized_weights = esb_quantize(weights, bits, k, alpha)\n",
        "\n",
        "error = np.mean((weights-quantized_weights)**2)\n",
        "# 打印结果\n",
        "print(f\"Original weights: {weights}\")\n",
        "print(f\"Quantized weights: {quantized_weights}\")\n",
        "print(f\"Error: {error}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe5BJzDiT3ra",
        "outputId": "8130f2a9-059d-4f35-fb7e-9c39f1f73a3a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original weights: [ 0.51338319  0.74506616  0.55793369  0.39568909 -0.8862254  -0.73647705\n",
            "  0.0612544   0.17728931 -0.08936226 -0.08005121]\n",
            "Quantized weights: [ 1.01282903  1.01282903  1.01282903  0.50641451 -1.01282903 -1.01282903\n",
            "  0.06330181  0.25320726 -0.12660363 -0.12660363]\n",
            "Error: 0.06420536571266794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_tokenizer(model_name):\n",
        "    model = OPTForCausalLM.from_pretrained(model_name)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def text_dataset():\n",
        "    test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "    return test_dataset\n",
        "\n",
        "def encode_dataset(tokenizer, testdata):\n",
        "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
        "    return testenc\n",
        "\n",
        "# class CalibrationDataset(Dataset):\n",
        "#     def __init__(self, tokenizer, data):\n",
        "#         self.encodings = tokenizer(\"\\n\\n\".join(data['text']), return_tensors='pt')\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         return {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings.input_ids)\n",
        "\n",
        "# def create_dataloader(tokenizer, data, batch_size):\n",
        "#     dataset = CalibrationDataset(tokenizer, data)\n",
        "#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "#     return dataloader"
      ],
      "metadata": {
        "id": "f7nYKU_wYUnR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quantization_error_weighted(weights, alpha, b, k, eps=1e-8):\n",
        "    quantized_weights = esb_quantize(weights, b, k, alpha)\n",
        "    abs_weights = np.abs(weights)\n",
        "    weights_sum = np.sum(abs_weights)\n",
        "    weighted_error = np.sum(np.abs(weights - quantized_weights) * abs_weights) / (weights_sum + eps)\n",
        "    return weighted_error\n",
        "\n",
        "def find_optimal_k_alpha(weights, b):\n",
        "    min_error = float(\"inf\")\n",
        "    optimal_k = None\n",
        "    optimal_alpha = None\n",
        "\n",
        "    for k in range(b-1,b):\n",
        "        alpha_opt = minimize_scalar(lambda a: quantization_error_weighted(weights, a, b, k)).x\n",
        "        error = quantization_error_weighted(weights, alpha_opt, b, k)\n",
        "        if error < min_error:\n",
        "            min_error = error\n",
        "            optimal_k = k\n",
        "            optimal_alpha = alpha_opt\n",
        "\n",
        "    return optimal_k, optimal_alpha, min_error\n",
        "\n",
        "def quantize_gpt2_layer(layer, b):\n",
        "    weights = layer.weight.cpu().detach().numpy()\n",
        "\n",
        "    optimal_k, optimal_alpha, min_error = find_optimal_k_alpha(weights, b)\n",
        "\n",
        "    quantized_weights = esb_quantize(weights, b, optimal_k, optimal_alpha)\n",
        "    layer.weight = torch.nn.Parameter(torch.tensor(quantized_weights))\n",
        "\n",
        "    print(f\"最优k: {optimal_k}, 最优α: {optimal_alpha:.4f}, 量化相对误差：{min_error:.8f}\")"
      ],
      "metadata": {
        "id": "dMSjILehYhuN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_layer_equalization(layer1, layer2):\n",
        "    weight1 = layer1.weight.cpu().detach().numpy()\n",
        "    weight2 = layer2.weight.cpu().detach().numpy()\n",
        "\n",
        "    scale1 = np.sqrt(np.mean(weight1**2))\n",
        "    scale2 = np.sqrt(np.mean(weight2**2))\n",
        "\n",
        "    layer1.weight = torch.nn.Parameter(torch.tensor(weight1 / scale1 * np.sqrt(scale1 * scale2)))\n",
        "    layer2.weight = torch.nn.Parameter(torch.tensor(weight2 / scale2 * np.sqrt(scale1 * scale2)))\n"
      ],
      "metadata": {
        "id": "YPiAyibZYkO-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize_opt_model(model, b):\n",
        "\n",
        "    start_time = time.time()\n",
        "    for i, layer in enumerate(model.model.decoder.layers):\n",
        "        print(f\"\\n对第{i+1}个Decoder Layer进行量化...\")\n",
        "\n",
        "        print(\"对注意力层进行量化...\")\n",
        "        # cross_layer_equalization(layer.self_attn.k_proj, layer.self_attn.q_proj)\n",
        "        # cross_layer_equalization(layer.self_attn.v_proj, layer.self_attn.out_proj)\n",
        "        quantize_gpt2_layer(layer.self_attn.k_proj, b)\n",
        "        quantize_gpt2_layer(layer.self_attn.v_proj, b)\n",
        "        quantize_gpt2_layer(layer.self_attn.q_proj, b)\n",
        "        quantize_gpt2_layer(layer.self_attn.out_proj, b)\n",
        "\n",
        "        print(\"对前馈神经网络层进行量化...\")\n",
        "        # cross_layer_equalization(layer.fc1, layer.fc2)\n",
        "        quantize_gpt2_layer(layer.fc1, b)\n",
        "        quantize_gpt2_layer(layer.fc2, b)\n",
        "\n",
        "        print(\"对Layer Norm层进行量化...\")\n",
        "        quantize_gpt2_layer(layer.self_attn_layer_norm, b)\n",
        "        quantize_gpt2_layer(layer.final_layer_norm, b)\n",
        "\n",
        "    # print(\"\\n对嵌入层进行量化...\")\n",
        "    # quantize_layer(copy_model.model.decoder.embed_tokens, b)\n",
        "    # quantize_layer(copy_model.model.decoder.embed_positions, b)\n",
        "\n",
        "    # print(\"\\n对最后的Layer Norm层进行量化...\")\n",
        "    # quantize_layer(copy_model.model.decoder.final_layer_norm, b)\n",
        "\n",
        "    # print(\"\\n对最后的线性层进行量化...\")\n",
        "    # quantize_layer(copy_model.lm_head, b)\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"量化总时间: {end_time - start_time:.2f} 秒\")"
      ],
      "metadata": {
        "id": "hEhslzhZYni5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "@torch.no_grad()\n",
        "# 评估OPT模型使用testenc数据和指定设备的性能\n",
        "def calculate_perplexity(model, testenc, dev):\n",
        "    '''\n",
        "    1.根据输入数据对OPT模型进行评估，评估性能指标，并计算模型的性能。\n",
        "    2.将指定的模型层移动到指定设备，并准备捕获输入数据和注意力掩码。\n",
        "    3.通过遍历模型的每个层，对每一层进行评估，根据评估结果计算性能指标，以便评估模型的性能表现。\n",
        "    '''\n",
        "    print('Evaluating ...')\n",
        "\n",
        "    testenc = testenc.input_ids\n",
        "    nsamples = testenc.numel() // 2048 # 根据testenc数据计算样本数\n",
        "\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.decoder.layers\n",
        "\n",
        "    # 将模型参数加载到指定设备\n",
        "    model.model.decoder.embed_tokens = model.model.decoder.embed_tokens.to(dev)\n",
        "    model.model.decoder.embed_positions = model.model.decoder.embed_positions.to(dev)\n",
        "    if hasattr(model.model.decoder, 'project_out') and model.model.decoder.project_out:\n",
        "        model.model.decoder.project_out = model.model.decoder.project_out.to(dev)\n",
        "    if hasattr(model.model.decoder, 'project_in') and model.model.decoder.project_in:\n",
        "        model.model.decoder.project_in = model.model.decoder.project_in.to(dev)\n",
        "    layers[0] = layers[0].to(dev)\n",
        "\n",
        "    dtype = next(iter(model.parameters())).dtype\n",
        "    inps = torch.zeros(\n",
        "        (nsamples, 2048, model.config.hidden_size), dtype=dtype, device=dev\n",
        "    )\n",
        "    cache = {'i': 0, 'attention_mask': None}\n",
        "\n",
        "    class Catcher(nn.Module):\n",
        "        def __init__(self, module):\n",
        "            super().__init__()\n",
        "            self.module = module\n",
        "        def forward(self, inp, **kwargs):\n",
        "            inps[cache['i']] = inp\n",
        "            cache['i'] += 1\n",
        "            cache['attention_mask'] = kwargs['attention_mask']\n",
        "            raise ValueError\n",
        "    layers[0] = Catcher(layers[0])\n",
        "    for i in range(nsamples):\n",
        "        batch = testenc[:, (i * 2048):((i + 1) * 2048)].to(dev)\n",
        "        try:\n",
        "            model(batch)\n",
        "        except ValueError:\n",
        "            pass\n",
        "    layers[0] = layers[0].module\n",
        "\n",
        "    layers[0] = layers[0].cpu()\n",
        "    model.model.decoder.embed_tokens = model.model.decoder.embed_tokens.cpu()\n",
        "    model.model.decoder.embed_positions = model.model.decoder.embed_positions.cpu()\n",
        "    if hasattr(model.model.decoder, 'project_out') and model.model.decoder.project_out:\n",
        "        model.model.decoder.project_out = model.model.decoder.project_out.cpu()\n",
        "    if hasattr(model.model.decoder, 'project_in') and model.model.decoder.project_in:\n",
        "        model.model.decoder.project_in = model.model.decoder.project_in.cpu()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    outs = torch.zeros_like(inps)\n",
        "    attention_mask = cache['attention_mask']\n",
        "\n",
        "    # 通过执行每个层并计算性能指标来进行评估\n",
        "    for i in range(len(layers)):\n",
        "        print(i)\n",
        "        # 对每一层进行评估，类似于opt_sequential函数\n",
        "        layer = layers[i].to(dev)\n",
        "        for j in range(nsamples):\n",
        "            outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
        "        layers[i] = layer.cpu()\n",
        "        del layer\n",
        "        torch.cuda.empty_cache()\n",
        "        inps, outs = outs, inps\n",
        "\n",
        "    if model.model.decoder.final_layer_norm is not None:\n",
        "        model.model.decoder.final_layer_norm = model.model.decoder.final_layer_norm.to(dev)\n",
        "    if model.model.decoder.project_out is not None:\n",
        "        model.model.decoder.project_out = model.model.decoder.project_out.to(dev)\n",
        "    model.lm_head = model.lm_head.to(dev)\n",
        "\n",
        "    testenc = testenc.to(dev)\n",
        "    nlls = []\n",
        "    for i in range(nsamples):\n",
        "        hidden_states = inps[i].unsqueeze(0)\n",
        "        if model.model.decoder.final_layer_norm is not None:\n",
        "            hidden_states = model.model.decoder.final_layer_norm(hidden_states)\n",
        "        if model.model.decoder.project_out is not None:\n",
        "            hidden_states = model.model.decoder.project_out(hidden_states)\n",
        "        lm_logits = model.lm_head(hidden_states)\n",
        "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
        "        shift_labels = testenc[\n",
        "            :, (i * 2048):((i + 1) * 2048)\n",
        "        ][:, 1:]\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "        neg_log_likelihood = loss.float() * 2048\n",
        "        nlls.append(neg_log_likelihood)\n",
        "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * 2048))\n",
        "    print(ppl.item())\n",
        "    return ppl.item()\n"
      ],
      "metadata": {
        "id": "vMmu3BdmZdCa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"facebook/opt-1.3b\"  # 可选: \"facebook/opt-125m\", \"facebook/opt-1.3b\", \"facebook/opt-2.7b\", \"facebook/opt-6.7b\", \"facebook/opt-13b\"\n",
        "\n",
        "model_13b, tokenizer = load_model_and_tokenizer(model_name)\n",
        "test_dataset = text_dataset()\n",
        "encodings = encode_dataset(tokenizer, test_dataset)"
      ],
      "metadata": {
        "id": "FA1O8JpYYrFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_model_4bit_13b = copy.deepcopy(model_13b)\n",
        "b = 4\n",
        "quantize_opt_model(copy_model_4bit_13b, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OePQq_65ZB5J",
        "outputId": "7f916c15-c032-453a-a577-159ff32bd58b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "对第1个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-526b1f98fe2e>:28: RuntimeWarning: divide by zero encountered in divide\n",
            "  scaled_weights[mask] = weights[mask] / alpha\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "最优k: 3, 最优α: 1.6445, 量化相对误差：0.00071445\n",
            "最优k: 3, 最优α: 1.6680, 量化相对误差：0.00033383\n",
            "最优k: 3, 最优α: 1.6339, 量化相对误差：0.00070471\n",
            "最优k: 3, 最优α: 1.0010, 量化相对误差：0.00048381\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 1.6953, 量化相对误差：0.00104645\n",
            "最优k: 3, 最优α: 1.3789, 量化相对误差：0.00095629\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第2个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6553, 量化相对误差：0.00132428\n",
            "最优k: 3, 最优α: 1.6907, 量化相对误差：0.00055173\n",
            "最优k: 3, 最优α: 1.3789, 量化相对误差：0.00124701\n",
            "最优k: 3, 最优α: 1.0000, 量化相对误差：0.00071087\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 1.6940, 量化相对误差：0.00124313\n",
            "最优k: 3, 最优α: 1.3893, 量化相对误差：0.00108872\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第3个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 0.8359, 量化相对误差：0.00141794\n",
            "最优k: 3, 最优α: 1.6914, 量化相对误差：0.00070961\n",
            "最优k: 3, 最优α: 0.8367, 量化相对误差：0.00136730\n",
            "最优k: 3, 最优α: 1.0010, 量化相对误差：0.00060921\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 1.6924, 量化相对误差：0.00123442\n",
            "最优k: 3, 最优α: 1.3916, 量化相对误差：0.00109799\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第4个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 0.8359, 量化相对误差：0.00136937\n",
            "最优k: 3, 最优α: 1.6966, 量化相对误差：0.00074966\n",
            "最优k: 3, 最优α: 0.8392, 量化相对误差：0.00138999\n",
            "最优k: 3, 最优α: 1.6047, 量化相对误差：0.00060519\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 1.6940, 量化相对误差：0.00115992\n",
            "最优k: 3, 最优α: 2.7839, 量化相对误差：0.00106585\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第5个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.1786, 量化相对误差：0.00139626\n",
            "最优k: 3, 最优α: 1.7009, 量化相对误差：0.00075748\n",
            "最优k: 3, 最优α: 0.8387, 量化相对误差：0.00149325\n",
            "最优k: 3, 最优α: 1.6003, 量化相对误差：0.00059965\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 1.6953, 量化相对误差：0.00110744\n",
            "最优k: 3, 最优α: 2.7891, 量化相对误差：0.00105759\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第6个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6439, 量化相对误差：0.00122933\n",
            "最优k: 3, 最优α: 1.7031, 量化相对误差：0.00079863\n",
            "最优k: 3, 最优α: 1.1774, 量化相对误差：0.00154642\n",
            "最优k: 3, 最优α: 1.6419, 量化相对误差：0.00061261\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 1.6951, 量化相对误差：0.00111038\n",
            "最优k: 3, 最优α: 2.7878, 量化相对误差：0.00107685\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第7个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6484, 量化相对误差：0.00141634\n",
            "最优k: 3, 最优α: 1.7063, 量化相对误差：0.00082960\n",
            "最优k: 3, 最优α: 0.8367, 量化相对误差：0.00169756\n",
            "最优k: 3, 最优α: 1.6826, 量化相对误差：0.00065297\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 1.6940, 量化相对误差：0.00112399\n",
            "最优k: 3, 最优α: 2.7891, 量化相对误差：0.00110802\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第8个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6497, 量化相对误差：0.00144684\n",
            "最优k: 3, 最优α: 1.7047, 量化相对误差：0.00083509\n",
            "最优k: 3, 最优α: 0.4196, 量化相对误差：0.00163036\n",
            "最优k: 3, 最优α: 0.8414, 量化相对误差：0.00066897\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 1.6929, 量化相对误差：0.00115435\n",
            "最优k: 3, 最优α: 2.7891, 量化相对误差：0.00115470\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第9个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6496, 量化相对误差：0.00146898\n",
            "最优k: 3, 最优α: 1.7047, 量化相对误差：0.00083209\n",
            "最优k: 3, 最优α: 0.5180, 量化相对误差：0.00163667\n",
            "最优k: 3, 最优α: 1.6766, 量化相对误差：0.00067257\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 1.6922, 量化相对误差：0.00117167\n",
            "最优k: 3, 最优α: 2.7872, 量化相对误差：0.00118042\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第10个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6558, 量化相对误差：0.00144307\n",
            "最优k: 3, 最优α: 1.7031, 量化相对误差：0.00082388\n",
            "最优k: 3, 最优α: 0.5176, 量化相对误差：0.00162511\n",
            "最优k: 3, 最优α: 1.6784, 量化相对误差：0.00068201\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 2.3843, 量化相对误差：0.00119718\n",
            "最优k: 3, 最优α: 2.7902, 量化相对误差：0.00116808\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第11个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6578, 量化相对误差：0.00140678\n",
            "最优k: 3, 最优α: 1.7063, 量化相对误差：0.00084744\n",
            "最优k: 3, 最优α: 0.6952, 量化相对误差：0.00147753\n",
            "最优k: 3, 最优α: 0.8437, 量化相对误差：0.00072779\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 2.0761, 量化相对误差：0.00120088\n",
            "最优k: 3, 最优α: 2.3795, 量化相对误差：0.00117401\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第12个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6628, 量化相对误差：0.00135092\n",
            "最优k: 3, 最优α: 1.7047, 量化相对误差：0.00087283\n",
            "最优k: 3, 最优α: 0.6947, 量化相对误差：0.00140316\n",
            "最优k: 3, 最优α: 1.6953, 量化相对误差：0.00075716\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 2.3828, 量化相对误差：0.00115936\n",
            "最优k: 3, 最优α: 2.3777, 量化相对误差：0.00115958\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第13个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6547, 量化相对误差：0.00130731\n",
            "最优k: 3, 最优α: 1.7109, 量化相对误差：0.00086857\n",
            "最优k: 3, 最优α: 0.5171, 量化相对误差：0.00140062\n",
            "最优k: 3, 最优α: 1.6953, 量化相对误差：0.00077862\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 2.7924, 量化相对误差：0.00109604\n",
            "最优k: 3, 最优α: 2.3773, 量化相对误差：0.00110622\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第14个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6484, 量化相对误差：0.00126843\n",
            "最优k: 3, 最优α: 1.7132, 量化相对误差：0.00086582\n",
            "最优k: 3, 最优α: 0.5176, 量化相对误差：0.00137073\n",
            "最优k: 3, 最优α: 1.7058, 量化相对误差：0.00079521\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 2.3796, 量化相对误差：0.00112059\n",
            "最优k: 3, 最优α: 2.7922, 量化相对误差：0.00109757\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第15个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6456, 量化相对误差：0.00123240\n",
            "最优k: 3, 最优α: 1.7078, 量化相对误差：0.00092396\n",
            "最优k: 3, 最优α: 1.6810, 量化相对误差：0.00125881\n",
            "最优k: 3, 最优α: 2.0703, 量化相对误差：0.00088102\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 2.7943, 量化相对误差：0.00111525\n",
            "最优k: 3, 最优α: 2.3789, 量化相对误差：0.00114175\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第16个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6609, 量化相对误差：0.00119841\n",
            "最优k: 3, 最优α: 1.7096, 量化相对误差：0.00099913\n",
            "最优k: 3, 最优α: 1.6859, 量化相对误差：0.00123462\n",
            "最优k: 3, 最优α: 1.6966, 量化相对误差：0.00093663\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 2.0742, 量化相对误差：0.00116767\n",
            "最优k: 3, 最优α: 4.7592, 量化相对误差：0.00115275\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第17个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6438, 量化相对误差：0.00112455\n",
            "最优k: 3, 最优α: 1.7148, 量化相对误差：0.00102145\n",
            "最优k: 3, 最优α: 1.6820, 量化相对误差：0.00115419\n",
            "最优k: 3, 最优α: 1.6953, 量化相对误差：0.00100315\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 2.0742, 量化相对误差：0.00119558\n",
            "最优k: 3, 最优α: 2.3815, 量化相对误差：0.00119437\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第18个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6426, 量化相对误差：0.00112511\n",
            "最优k: 3, 最优α: 1.7122, 量化相对误差：0.00105223\n",
            "最优k: 3, 最优α: 1.6853, 量化相对误差：0.00114373\n",
            "最优k: 3, 最优α: 2.3815, 量化相对误差：0.00104131\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 2.3789, 量化相对误差：0.00121140\n",
            "最优k: 3, 最优α: 2.3816, 量化相对误差：0.00124572\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第19个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6609, 量化相对误差：0.00110427\n",
            "最优k: 3, 最优α: 1.7063, 量化相对误差：0.00110709\n",
            "最优k: 3, 最优α: 1.6778, 量化相对误差：0.00112079\n",
            "最优k: 3, 最优α: 2.3741, 量化相对误差：0.00110952\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 2.3796, 量化相对误差：0.00124241\n",
            "最优k: 3, 最优α: 2.3809, 量化相对误差：0.00128828\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第20个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6524, 量化相对误差：0.00111432\n",
            "最优k: 3, 最优α: 1.7047, 量化相对误差：0.00126385\n",
            "最优k: 3, 最优α: 1.6784, 量化相对误差：0.00112820\n",
            "最优k: 3, 最优α: 0.5180, 量化相对误差：0.00126542\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 2.0742, 量化相对误差：0.00131163\n",
            "最优k: 3, 最优α: 2.3796, 量化相对误差：0.00134668\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第21个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6455, 量化相对误差：0.00111987\n",
            "最优k: 3, 最优α: 1.6984, 量化相对误差：0.00135866\n",
            "最优k: 3, 最优α: 1.6734, 量化相对误差：0.00112242\n",
            "最优k: 3, 最优α: 0.8421, 量化相对误差：0.00131223\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 2.0742, 量化相对误差：0.00134954\n",
            "最优k: 3, 最优α: 2.0749, 量化相对误差：0.00140011\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第22个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6628, 量化相对误差：0.00117959\n",
            "最优k: 3, 最优α: 2.0762, 量化相对误差：0.00147184\n",
            "最优k: 3, 最优α: 1.6728, 量化相对误差：0.00119726\n",
            "最优k: 3, 最优α: 1.6797, 量化相对误差：0.00137198\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 2.0742, 量化相对误差：0.00138407\n",
            "最优k: 3, 最优α: 2.0742, 量化相对误差：0.00145059\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第23个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.6686, 量化相对误差：0.00126247\n",
            "最优k: 3, 最优α: 2.7934, 量化相对误差：0.00153733\n",
            "最优k: 3, 最优α: 1.6720, 量化相对误差：0.00135079\n",
            "最优k: 3, 最优α: 1.6687, 量化相对误差：0.00145874\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 2.0750, 量化相对误差：0.00139952\n",
            "最优k: 3, 最优α: 2.7839, 量化相对误差：0.00145202\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "\n",
            "对第24个Decoder Layer进行量化...\n",
            "对注意力层进行量化...\n",
            "最优k: 3, 最优α: 1.4754, 量化相对误差：0.00146247\n",
            "最优k: 3, 最优α: 2.3863, 量化相对误差：0.00126925\n",
            "最优k: 3, 最优α: 1.0000, 量化相对误差：0.00233152\n",
            "最优k: 3, 最优α: 1.6438, 量化相对误差：0.00139903\n",
            "对前馈神经网络层进行量化...\n",
            "最优k: 3, 最优α: 1.6966, 量化相对误差：0.00132628\n",
            "最优k: 3, 最优α: 2.7872, 量化相对误差：0.00154891\n",
            "对Layer Norm层进行量化...\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "最优k: 3, 最优α: 4.0000, 量化相对误差：0.00000000\n",
            "量化总时间: 2916.58 秒\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# ppl = calculate_perplexity(model, encodings)\n",
        "ppl = calculate_perplexity(model, encodings, dev)\n",
        "\n",
        "print(f\"困惑度(PPL): {ppl}\")"
      ],
      "metadata": {
        "id": "6qBQOgEAZGF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# ppl = calculate_perplexity(model, encodings)\n",
        "ppl = calculate_perplexity(copy_model_4bit_13b, encodings, dev)\n",
        "print(f\"{model_name}量化到{b}比特的困惑度(PPL): {ppl}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihvDO9zBbBqR",
        "outputId": "939fc501-2db7-4415-96a7-d3c3a2bc8aec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating ...\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "14.700271606445312\n",
            "facebook/opt-1.3b量化到4比特的困惑度(PPL): 14.700271606445312\n"
          ]
        }
      ]
    }
  ]
}