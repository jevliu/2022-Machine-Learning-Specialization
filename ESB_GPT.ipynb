{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBGEDy1JsGmneOVo4nMMk4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jevliu/2022-Machine-Learning-Specialization/blob/main/ESB_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 针对GPT-2模型进行ESB量化"
      ],
      "metadata": {
        "id": "WP0uPPL5EMjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 加载必要的库"
      ],
      "metadata": {
        "id": "pSCd-wXoDCMg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ5TQ-iFCblh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize_scalar\n",
        "from scipy.stats import laplace\n",
        "import math\n",
        "import time\n",
        "from transformers import GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 定义量化函数"
      ],
      "metadata": {
        "id": "J-77KL9yDKq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def esb_quantize(weights, b, k, alpha, eps=1e-8):\n",
        "    # 计算N\n",
        "    N = 2 ** (b - k - 1) - 1\n",
        "\n",
        "    # 生成移位因子xi\n",
        "    xi = [2 ** -k] + [2 ** (i - k - 1) for i in range(1, N + 1)]\n",
        "\n",
        "    # 生成分数集合Omega\n",
        "    Omega = [[0] + list(range(1, 2 ** k))] + [list(range(2 ** k, 2 ** (k + 1))) for _ in range(N)]\n",
        "\n",
        "    # 生成非负的量化值集合Q_positive\n",
        "    Q_positive = []\n",
        "    for i in range(N + 1):\n",
        "        Q_positive.extend([alpha * x * y for x in xi[i:] for y in Omega[i]])\n",
        "    Q_positive = sorted(set(Q_positive))\n",
        "\n",
        "    # 对量化值集合进行对称处理\n",
        "    Q_e = sorted(set([-q for q in Q_positive] + [0] + Q_positive))\n",
        "\n",
        "    # 计算量化范围C\n",
        "    C = np.max(Q_e)\n",
        "\n",
        "    # 创建掩码,标记非零权重\n",
        "    mask = weights != 0\n",
        "\n",
        "    # 缩放非零权重\n",
        "    scaled_weights = np.zeros_like(weights)\n",
        "    scaled_weights[mask] = weights[mask] / alpha\n",
        "\n",
        "    # 截断非零权重\n",
        "    clipped_weights = np.clip(scaled_weights, -C, C)\n",
        "\n",
        "    # 移位量化非零权重\n",
        "    quantized_weights = np.zeros_like(clipped_weights)\n",
        "    nonzero_indices = np.where(mask)\n",
        "    v = clipped_weights[nonzero_indices]\n",
        "    n = np.ceil(np.log2(np.abs(v) + eps))\n",
        "    quantized_v = np.round(v / (2 ** (n - k))) * (2 ** (n - k))\n",
        "    quantized_weights[nonzero_indices] = quantized_v\n",
        "\n",
        "    # 反缩放量化后的权重\n",
        "    dequantized_weights = quantized_weights * alpha\n",
        "\n",
        "    return dequantized_weights"
      ],
      "metadata": {
        "id": "vW88JHvLCePz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 定义加载模型和数据集的函数"
      ],
      "metadata": {
        "id": "JMcscfcDDWru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_tokenizer(model_name):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def text_dataset():\n",
        "    test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "    return test_dataset\n",
        "\n",
        "def encode_dataset(tokenizer, testdata):\n",
        "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
        "    return testenc\n"
      ],
      "metadata": {
        "id": "VJGUj9e2DblY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 定义加权量化误差函数和寻找最优的k和α的函数"
      ],
      "metadata": {
        "id": "re_AQXXxD3W2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quantization_error_weighted(weights, alpha, b, k, eps=1e-8):\n",
        "    quantized_weights = esb_quantize(weights, b, k, alpha)\n",
        "    abs_weights = np.abs(weights)\n",
        "    weights_sum = np.sum(abs_weights)\n",
        "    weighted_error = np.sum(np.abs(weights - quantized_weights) * abs_weights) / (weights_sum + eps)\n",
        "    return weighted_error\n",
        "\n",
        "def find_optimal_k_alpha(weights, b):\n",
        "    min_error = float(\"inf\")\n",
        "    optimal_k = None\n",
        "    optimal_alpha = None\n",
        "\n",
        "    for k in range(b):\n",
        "        alpha_opt = minimize_scalar(lambda a: quantization_error_weighted(weights, a, b, k)).x\n",
        "        error = quantization_error_weighted(weights, alpha_opt, b, k)\n",
        "        if error < min_error:\n",
        "            min_error = error\n",
        "            optimal_k = k\n",
        "            optimal_alpha = alpha_opt\n",
        "\n",
        "    return optimal_k, optimal_alpha, min_error\n",
        "\n",
        "def quantize_gpt2_layer(layer, b):\n",
        "    weights = layer.weight.cpu().detach().numpy()\n",
        "\n",
        "    optimal_k, optimal_alpha, min_error = find_optimal_k_alpha(weights, b)\n",
        "\n",
        "    quantized_weights = esb_quantize(weights, b, optimal_k, optimal_alpha)\n",
        "    layer.weight = torch.nn.Parameter(torch.tensor(quantized_weights))\n",
        "\n",
        "    print(f\"最优k: {optimal_k}, 最优α: {optimal_alpha:.4f}, 量化相对误差：{min_error:.8f}\")"
      ],
      "metadata": {
        "id": "ClWyV7eLD3t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 针对模型进行量化的函数（只量化了Block）"
      ],
      "metadata": {
        "id": "lS9S2PfCETGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize_gpt_model(model, b):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 对嵌入层进行量化\n",
        "    # print(\"对嵌入层进行量化...\")\n",
        "    # quantize_gpt2_layer(quantized_model_2.transformer.wte, b)\n",
        "    # quantize_gpt2_layer(quantized_model_2.transformer.wpe, b)\n",
        "\n",
        "    # 对每个Block进行量化\n",
        "    for i, block in enumerate(model.transformer.h):\n",
        "        print(f\"\\n对第{i+1}个Block进行量化...\")\n",
        "\n",
        "        print(\"对注意力层进行量化...\")\n",
        "        quantize_gpt2_layer_new(block.attn.c_attn, b)\n",
        "        quantize_gpt2_layer_new(block.attn.c_proj, b)\n",
        "\n",
        "        print(\"对前馈层进行量化...\")\n",
        "        quantize_gpt2_layer_new(block.mlp.c_fc, b)\n",
        "        quantize_gpt2_layer_new(block.mlp.c_proj, b)\n",
        "\n",
        "        print(\"对LayerNorm层进行量化...\")\n",
        "        quantize_gpt2_layer_new(block.ln_1, b)\n",
        "        quantize_gpt2_layer_new(block.ln_2, b)\n",
        "\n",
        "    # 对最后的LayerNorm层和语言模型头进行量化\n",
        "    # print(\"对最后的LayerNorm层和语言模型头进行量化...\")\n",
        "    # quantize_gpt2_layer(quantized_model_2.transformer.ln_f, b)\n",
        "    # quantize_gpt2_layer(quantized_model_2.lm_head, b)\n",
        "    end_time = time.time()\n",
        "    print(f\"量化总时间: {end_time - start_time:.2f} 秒\")"
      ],
      "metadata": {
        "id": "a6i3Mbi2ETd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 定义计算困惑度的函数"
      ],
      "metadata": {
        "id": "zzPt1TluE-wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算困惑度的函数\n",
        "def compute_perplexity(model, data, device):\n",
        "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    data.to(device)\n",
        "\n",
        "    max_length = model.config.n_positions\n",
        "    stride = 512\n",
        "\n",
        "    lls = []\n",
        "    total_time = 0\n",
        "    for i in range(0, data.input_ids.size(1), stride):\n",
        "        begin_loc = max(i + stride - max_length, 0)\n",
        "        end_loc = min(i + stride, data.input_ids.size(1))\n",
        "        trg_len = end_loc - i\n",
        "        input_ids = data.input_ids[:, begin_loc:end_loc].to(device)\n",
        "        target_ids = input_ids.clone()\n",
        "        target_ids[:, :-trg_len] = -100\n",
        "\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, labels=target_ids)\n",
        "            log_likelihood = outputs[0] * trg_len\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        total_time += elapsed_time\n",
        "\n",
        "        lls.append(log_likelihood)\n",
        "\n",
        "    ppl = torch.exp(torch.stack(lls).sum() / end_loc)\n",
        "    print(f\"总前向传播时间: {total_time:.2f} 秒\")\n",
        "    return ppl.item()"
      ],
      "metadata": {
        "id": "jKhUgKTSE-JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 加载模型和测试数据"
      ],
      "metadata": {
        "id": "MSxa3eq0F5IT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"  # 可选: \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"\n",
        "\n",
        "model_125m, tokenizer = load_model_and_tokenizer(model_name)\n",
        "test_dataset = text_dataset()\n",
        "encodings = encode_dataset(tokenizer, test_dataset)"
      ],
      "metadata": {
        "id": "Ry-Kaa4PF5eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 指定量化位宽b并进行量化"
      ],
      "metadata": {
        "id": "EzMVIU9OGfxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "copy_model_4bit_125m = copy.deepcopy(model_125m)\n",
        "b = 4\n",
        "quantize_opt_model(copy_model_4bit_125m, b)"
      ],
      "metadata": {
        "id": "Jf9c-szpGgF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 评估量化后的困惑度"
      ],
      "metadata": {
        "id": "Re3e-GZFGiXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ppl = calculate_perplexity(copy_model_4bit_125m, encodings, dev)\n",
        "print(f\"{model_name}量化到{b}比特的困惑度(PPL): {ppl}\")"
      ],
      "metadata": {
        "id": "6RWFgPM_Gl0Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}