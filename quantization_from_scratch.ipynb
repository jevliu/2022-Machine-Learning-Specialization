{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPv9jFk66Qt+FheuUOwbNC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jevliu/2022-Machine-Learning-Specialization/blob/main/quantization_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PPmBJYhw4sA7"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import terminal_size\n",
        "# suppres scitntific notation\n",
        "np.set_printoptions(suppress=True)"
      ],
      "metadata": {
        "id": "cNZPPo8u49D7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate random distributed parameters\n",
        "params = np.random.uniform(low=-50, high=150, size=20)\n",
        "\n",
        "# make sure importment values are at the begining for better debugging\n",
        "params[0] = params.max() + 1\n",
        "params[1] = params.min() - 1\n",
        "params[2] = 0\n",
        "\n",
        "# round each number to the second decimal place\n",
        "params = np.round(params, 2)\n",
        "\n",
        "# print the parameters\n",
        "print(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVIdOVth5VpR",
        "outputId": "6a7503dc-a5d4-41f4-dc45-ceb6e8342028"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[149.28 -17.66   0.    61.84  -0.25 134.6  148.28  68.34  74.51  51.24\n",
            "  43.13 140.25 142.08 114.53 -16.66  81.33 135.17  -8.6  112.6  103.86]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define several function for quantization and dequantize according to the\n",
        "# mathmetical formular\n",
        "def clamp(param_q:np.array, lower_bound:int, upper_bound:int)->np.array:\n",
        "  param_q[param_q < lower_bound] = lower_bound\n",
        "  param_q[param_q > upper_bound] = upper_bound\n",
        "  return param_q\n",
        "\n",
        "def asymmetric_quantization(params:np.array,bits:int)->tuple[np.array,float,int]:\n",
        "  # calulate the scale and zero point\n",
        "  alpha = np.max(params)\n",
        "  beta = np.min(params)\n",
        "  scale = (alpha-beta) / (2**bits-1)\n",
        "  zero = -1*np.round(beta/scale)\n",
        "  # unsigned integer\n",
        "  lower_bound, upper_bound = 0, 2**bits-1\n",
        "  # quantize the parameters\n",
        "  quantized = clamp(np.round(params/scale+zero),lower_bound,upper_bound).astype(np.int32)\n",
        "  return quantized,scale,zero\n",
        "\n",
        "def symmetric_quantization(params:np.array,bits:int)->tuple[np.array,float]:\n",
        "  # calculate the scale\n",
        "  alpha = np.max(np.abs(params))\n",
        "  scale = alpha / (2**(bits-1)-1)\n",
        "  lower_bound,upper_bound = -1*(2**(bits-1)),2**(bits-1)-1\n",
        "  quantized = clamp(np.round(params/scale),lower_bound,upper_bound).astype(np.int32)\n",
        "  return quantized,scale\n",
        "\n",
        "def asymmetric_dequantize(params:np.array,scale:float,zero:int)->np.array:\n",
        "  return scale * (params-zero)\n",
        "\n",
        "def symmetric_dequantize(params:np.array,scale:float)->np.array:\n",
        "  return scale * params\n",
        "\n",
        "def quantization_error(params:np.array, params_q:np.array):\n",
        "  # calculate the MSE\n",
        "  return np.mean((params-params_q)**2)\n"
      ],
      "metadata": {
        "id": "8vYRnUBu6bDm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(asymmetric_q, asymmetric_s, asymmetric_z) = asymmetric_quantization(params, 8)\n",
        "(symmetric_q, symmetric_s) = symmetric_quantization(params, 8)\n",
        "as_deq_params = asymmetric_dequantize(asymmetric_q, asymmetric_s, asymmetric_z)\n",
        "sy_deq_params = symmetric_dequantize(symmetric_q, symmetric_s)\n",
        "\n",
        "print('original parameters:\\n',np.round(params,2))\n",
        "print('parameters after asymmetric quantitation:\\n',np.round(asymmetric_q))\n",
        "print(f'asymmetric_scale: {np.round(asymmetric_s,2)}, asymmetric_zero: {asymmetric_z.round(2)}')\n",
        "print('parameters after symmetric quantitation:\\n',np.round(symmetric_q))\n",
        "print(f'symmetric_scale: {symmetric_s.round(2)}')\n",
        "print(f'quantitation error with asymmetric: {quantization_error(params,as_deq_params).round(2)}')\n",
        "print(f'quantitation error with symmetric: {quantization_error(params,sy_deq_params).round(2)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrKCKkiKAh_H",
        "outputId": "a0625458-2e70-428b-fafd-10fa2b07bac8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original parameters:\n",
            " [149.28 -17.66   0.    61.84  -0.25 134.6  148.28  68.34  74.51  51.24\n",
            "  43.13 140.25 142.08 114.53 -16.66  81.33 135.17  -8.6  112.6  103.86]\n",
            "parameters after asymmetric quantitation:\n",
            " [255   0  27 121  27 233 253 131 141 105  93 241 244 202   2 151 233  14\n",
            " 199 186]\n",
            "asymmetric_scale: 0.65, asymmetric_zero: 27.0\n",
            "parameters after symmetric quantitation:\n",
            " [127 -15   0  53   0 115 126  58  63  44  37 119 121  97 -14  69 115  -7\n",
            "  96  88]\n",
            "symmetric_scale: 1.18\n",
            "quantitation error with asymmetric: 0.04\n",
            "quantitation error with symmetric: 0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization range:how to choose alpha&beta\n",
        "\n",
        "### Quantization strategy\n",
        "\n",
        "**Min-Max:** sensitive to outlier numbers\n",
        "\n",
        "**Percntile:** only the outlier has big error\n",
        "\n",
        "**Mean-Square-Error:**It is usually solved using Grid-Search\n",
        "\n",
        "**Cross-Entropy:**used when the values in the tensor being quantized are not equally importan.\n",
        "to keep the order in the softmax layer\n",
        "\n",
        "### Quantization granularity"
      ],
      "metadata": {
        "id": "xz9jBzreNGxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post Tranining Quantization (PTQ)\n",
        "\n",
        "### PTQ process:\n",
        "\n",
        "pre-trained model --> attatch observers(calculate the s and z parameter using the observed data) --> calibrate --> quantized model"
      ],
      "metadata": {
        "id": "1MJzto8uQfyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import os"
      ],
      "metadata": {
        "id": "Iq5KXmHAQlrx"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load the MNIST dataset"
      ],
      "metadata": {
        "id": "05ketPzoRzfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make torch deterministic\n",
        "_ = torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "b7iqOpz4RTVy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,),(0.3081,))])\n",
        "\n",
        "# load the MNIST dataset\n",
        "mnist_trainset = datasets.MNIST(root='./data',train=True,download=True,transform=transform)\n",
        "# create a dataloader for the training\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset,batch_size=10,shuffle=True)\n",
        "\n",
        "# load the MNIST test dataset\n",
        "mnist_testset = datasets.MNIST(root='./data',train=False,download=True,transform=transform)\n",
        "# create a dataloader for the testing\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testset,batch_size=10,shuffle=True)\n",
        "\n",
        "# define the device\n",
        "device = 'cpu'"
      ],
      "metadata": {
        "id": "iYi2GbziSaaZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the model"
      ],
      "metadata": {
        "id": "Lom6Q8GOUd_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VerySimpleNet(nn.Module):\n",
        "  def __init__(self,hidden_size_1=100,hidden_size_2=100):\n",
        "    super(VerySimpleNet,self).__init__()\n",
        "    self.linear1 = nn.Linear(28*28,hidden_size_1)\n",
        "    self.linear2= nn.Linear(hidden_size_1,hidden_size_2)\n",
        "    self.linear3 = nn.Linear(hidden_size_2,10)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self,img):\n",
        "    x = img.view(-1,28*28)\n",
        "    x = self.relu(self.linear1(x))\n",
        "    x = self.relu(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "SHvV_c2LS3of"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = VerySimpleNet().to(device)"
      ],
      "metadata": {
        "id": "xBo7cGPHVmCH"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model"
      ],
      "metadata": {
        "id": "Bq0XDjLsVxV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader,net,epochs=5,total_iterations_limit=None):\n",
        "  cross_el = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "  total_iterations = 0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    net.train()\n",
        "\n",
        "    loss_sum = 0\n",
        "    num_iterations = 0\n",
        "\n",
        "    data_iterator = tqdm(train_loader,desc=f'Epoch {epoch+1}')\n",
        "    for data in data_iterator:\n",
        "      num_iterations += 1\n",
        "      total_iterations += 1\n",
        "      x, y = data\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output = net(x.view(-1,28*28))\n",
        "      loss = cross_el(output,y)\n",
        "      loss_sum += loss\n",
        "      avg_loss = loss_sum / num_iterations\n",
        "      data_iterator.set_postfix(loss=avg_loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "def print_size_of_model(model):\n",
        "  torch.save(model.state_dict(),\"temp_delme.p\")\n",
        "  print(\"Size (KB):\", os.path.getsize(\"temp_delme.p\")/1e3)\n",
        "  os.remove('temp_delme.p')\n",
        "\n",
        "MODEL_FILENAME = 'simplenet_ptq_pt'\n",
        "\n",
        "if Path(MODEL_FILENAME).exists():\n",
        "  net.load_state_dict(torch.load(MODEL_FILENAME))\n",
        "  print('Loaded model from disk')\n",
        "else:\n",
        "  train(train_loader,net,epochs=1)\n",
        "  # save the model tp disk\n",
        "  torch.save(net.state_dict(), MODEL_FILENAME)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fq6kZuwCVwAa",
        "outputId": "e8c07d42-80ea-43e1-df2f-a26e12ac1bef"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model from disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the testing loop"
      ],
      "metadata": {
        "id": "3Z0vCeQaafjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model:nn.Module,total_iterations:int=None):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  iterations = 0\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in tqdm(test_loader,desc='Testing'):\n",
        "      x, y = data\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      output = model(x.view(-1,784))\n",
        "      for idx,i in enumerate(output):\n",
        "        if torch.argmax(i) == y[idx]:\n",
        "          correct += 1\n",
        "        total += 1\n",
        "      iterations += 1\n",
        "      if total_iterations is not None and iterations >= total_iterations:\n",
        "        break\n",
        "  print(f'Accuracy: {round(correct/total, 3)}')"
      ],
      "metadata": {
        "id": "rhOSNxlhYaPh"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print weights and size of the model before quantization"
      ],
      "metadata": {
        "id": "U_tGwn21cNOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Weights before quantization')\n",
        "print(net.linear1.weight)\n",
        "print(net.linear1.weight.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTBWh0bscJwn",
        "outputId": "2928b469-10b5-446c-cddb-df9745474b69"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights before quantization\n",
            "Parameter containing:\n",
            "tensor([[ 0.0476,  0.0066,  0.0505,  ...,  0.0176,  0.0215,  0.0393],\n",
            "        [ 0.0074, -0.0103, -0.0235,  ...,  0.0176,  0.0246, -0.0121],\n",
            "        [ 0.0327,  0.0203,  0.0466,  ...,  0.0046,  0.0356,  0.0435],\n",
            "        ...,\n",
            "        [-0.0280, -0.0063, -0.0346,  ..., -0.0222, -0.0336, -0.0100],\n",
            "        [ 0.0332, -0.0112, -0.0090,  ..., -0.0258,  0.0404, -0.0072],\n",
            "        [ 0.0512,  0.0453,  0.0502,  ..., -0.0042,  0.0484,  0.0278]],\n",
            "       requires_grad=True)\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Size of the model before quantization')\n",
        "print_size_of_model(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMbRqq1ocl0d",
        "outputId": "0fe2eebe-ee02-4729-a7ba-b8550c16779e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the model before quantization\n",
            "Size (KB): 360.998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Accuracy of the model before quantization')\n",
        "test(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcGfF1d9iZtj",
        "outputId": "6b4f3918-ced0-4cf4-aa31-9354d58a507b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model before quantization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 1000/1000 [00:03<00:00, 307.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insert Min-Max observers in the model"
      ],
      "metadata": {
        "id": "csmH1Qz6dJ6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedVerySimpleNet(nn.Module):\n",
        "  def __init__(self,hidden_size_1=100,hidden_size_2=100):\n",
        "    super(QuantizedVerySimpleNet,self).__init__()\n",
        "    self.quant = torch.quantization.QuantStub()\n",
        "    self.linear1 = nn.Linear(28*28,hidden_size_1)\n",
        "    self.linear2= nn.Linear(hidden_size_1,hidden_size_2)\n",
        "    self.linear3 = nn.Linear(hidden_size_2,10)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "  def forward(self,img):\n",
        "    x = img.view(-1,28*28)\n",
        "    x = self.quant(x)\n",
        "    x = self.relu(self.linear1(x))\n",
        "    x = self.relu(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    x = self.dequant(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "j-99tNVYc5Cy"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_quantized = QuantizedVerySimpleNet().to(device)\n",
        "# copy weights from unquantized model\n",
        "net_quantized.load_state_dict(net.state_dict())\n",
        "net_quantized.eval()\n",
        "\n",
        "net_quantized.qconfig = torch.ao.quantization.default_qconfig\n",
        "net_quantized = torch.ao.quantization.prepare(net_quantized) # insert observers\n",
        "net_quantized\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP9Q0Dv5j9nI",
        "outputId": "9a93f20b-7f03-4930-ad60-1c0f17d611c1"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantizedVerySimpleNet(\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (linear1): Linear(\n",
              "    in_features=784, out_features=100, bias=True\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (linear2): Linear(\n",
              "    in_features=100, out_features=100, bias=True\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (linear3): Linear(\n",
              "    in_features=100, out_features=10, bias=True\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (relu): ReLU()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calibrate the model using the test set"
      ],
      "metadata": {
        "id": "IKJAlZAak-54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test(net_quantized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-01ObFOkvF-",
        "outputId": "fb1de7c6-b0a3-40d5-9d6f-4e307734de0b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 1000/1000 [00:03<00:00, 272.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Check statistics of the various layers')\n",
        "net_quantized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-Avc_6zlHyj",
        "outputId": "487d6495-d6eb-426b-98a0-2a2d186a8681"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check statistics of the various layers\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantizedVerySimpleNet(\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): MinMaxObserver(min_val=-0.4242129623889923, max_val=2.821486711502075)\n",
              "  )\n",
              "  (linear1): Linear(\n",
              "    in_features=784, out_features=100, bias=True\n",
              "    (activation_post_process): MinMaxObserver(min_val=-46.66990280151367, max_val=30.96890640258789)\n",
              "  )\n",
              "  (linear2): Linear(\n",
              "    in_features=100, out_features=100, bias=True\n",
              "    (activation_post_process): MinMaxObserver(min_val=-24.895591735839844, max_val=23.977750778198242)\n",
              "  )\n",
              "  (linear3): Linear(\n",
              "    in_features=100, out_features=10, bias=True\n",
              "    (activation_post_process): MinMaxObserver(min_val=-30.03553581237793, max_val=20.859119415283203)\n",
              "  )\n",
              "  (relu): ReLU()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantize the model using the statistics collected"
      ],
      "metadata": {
        "id": "73JCp-LrlbXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_quantized = torch.ao.quantization.convert(net_quantized)"
      ],
      "metadata": {
        "id": "lQS-JmQ6lRaD"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Check statistics of the various layers')\n",
        "net_quantized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwODD17AltmY",
        "outputId": "95cd3cba-5ef3-4da0-8b17-177f2d4b1096"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check statistics of the various layers\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantizedVerySimpleNet(\n",
              "  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)\n",
              "  (linear1): QuantizedLinear(in_features=784, out_features=100, scale=0.611329197883606, zero_point=76, qscheme=torch.per_tensor_affine)\n",
              "  (linear2): QuantizedLinear(in_features=100, out_features=100, scale=0.38482949137687683, zero_point=65, qscheme=torch.per_tensor_affine)\n",
              "  (linear3): QuantizedLinear(in_features=100, out_features=10, scale=0.40074530243873596, zero_point=75, qscheme=torch.per_tensor_affine)\n",
              "  (relu): ReLU()\n",
              "  (dequant): DeQuantize()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print the weights matrix of the model after quantizaion"
      ],
      "metadata": {
        "id": "ixkmDLJFmFP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Weights after quantization')\n",
        "print(torch.int_repr(net_quantized.linear1.weight()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Moen-oe5l2sF",
        "outputId": "315b075b-7eee-4134-c7dd-9ed1b4f4a855"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights after quantization\n",
            "tensor([[12,  2, 13,  ...,  4,  5, 10],\n",
            "        [ 2, -3, -6,  ...,  4,  6, -3],\n",
            "        [ 8,  5, 12,  ...,  1,  9, 11],\n",
            "        ...,\n",
            "        [-7, -2, -9,  ..., -6, -8, -2],\n",
            "        [ 8, -3, -2,  ..., -6, 10, -2],\n",
            "        [13, 11, 12,  ..., -1, 12,  7]], dtype=torch.int8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare the dequantized weights and the original weights"
      ],
      "metadata": {
        "id": "XT8N9XSUmZ1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Original weights')\n",
        "print(net.linear1.weight)\n",
        "print('')\n",
        "print('Dequantized weights')\n",
        "print(torch.dequantize(net_quantized.linear1.weight()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To5VPh5WmU0g",
        "outputId": "5bde5f9c-0fb5-47c5-aa36-f5de3b7173e1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original weights\n",
            "Parameter containing:\n",
            "tensor([[ 0.0476,  0.0066,  0.0505,  ...,  0.0176,  0.0215,  0.0393],\n",
            "        [ 0.0074, -0.0103, -0.0235,  ...,  0.0176,  0.0246, -0.0121],\n",
            "        [ 0.0327,  0.0203,  0.0466,  ...,  0.0046,  0.0356,  0.0435],\n",
            "        ...,\n",
            "        [-0.0280, -0.0063, -0.0346,  ..., -0.0222, -0.0336, -0.0100],\n",
            "        [ 0.0332, -0.0112, -0.0090,  ..., -0.0258,  0.0404, -0.0072],\n",
            "        [ 0.0512,  0.0453,  0.0502,  ..., -0.0042,  0.0484,  0.0278]],\n",
            "       requires_grad=True)\n",
            "\n",
            "Dequantized weights\n",
            "tensor([[ 0.0483,  0.0081,  0.0523,  ...,  0.0161,  0.0201,  0.0403],\n",
            "        [ 0.0081, -0.0121, -0.0242,  ...,  0.0161,  0.0242, -0.0121],\n",
            "        [ 0.0322,  0.0201,  0.0483,  ...,  0.0040,  0.0362,  0.0443],\n",
            "        ...,\n",
            "        [-0.0282, -0.0081, -0.0362,  ..., -0.0242, -0.0322, -0.0081],\n",
            "        [ 0.0322, -0.0121, -0.0081,  ..., -0.0242,  0.0403, -0.0081],\n",
            "        [ 0.0523,  0.0443,  0.0483,  ..., -0.0040,  0.0483,  0.0282]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print the size and accuracy of the quantized model"
      ],
      "metadata": {
        "id": "vM7goRMlnDbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Size of the model after quantization')\n",
        "print_size_of_model(net_quantized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dSKds2Dm01W",
        "outputId": "85f20953-07d0-4da7-f82d-bd72f0726eb3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the model after quantization\n",
            "Size (KB): 95.394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Testing the model after quantization')\n",
        "test(net_quantized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAhehLNknRXw",
        "outputId": "3a2802eb-918e-437a-b926-b8b92b17e5b8"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the model after quantization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 1000/1000 [00:03<00:00, 325.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization Aware Training (QAT)\n",
        "\n",
        "We insert some fake modules in the computational graph of the model to simulate the effect of the quantization during training\n",
        "\n",
        "This way,the loss function gets used to update the weights that constantly suffer from the effect of quantizaion,and it usually leads to a more robust model"
      ],
      "metadata": {
        "id": "5JT_DMOOoG-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VerySimpleNet2(nn.Module):\n",
        "  def __init__(self,hidden_size_1=100,hidden_size_2=100):\n",
        "    super(VerySimpleNet2,self).__init__()\n",
        "    self.quant = torch.quantization.QuantStub()\n",
        "    self.linear1 = nn.Linear(28*28,hidden_size_1)\n",
        "    self.linear2= nn.Linear(hidden_size_1,hidden_size_2)\n",
        "    self.linear3 = nn.Linear(hidden_size_2,10)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "  def forward(self,img):\n",
        "    x = img.view(-1,28*28)\n",
        "    x = self.quant(x)\n",
        "    x = self.relu(self.linear1(x))\n",
        "    x = self.relu(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    x = self.dequant(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "OYoqpJIUnbYO"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insert min-max observers in the model"
      ],
      "metadata": {
        "id": "lrrx9mIkpgHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net2 = VerySimpleNet2().to(device)\n",
        "net2.qconfig = torch.ao.quantization.default_qconfig\n",
        "net2.train()\n",
        "net_quantized_qwt = torch.ao.quantization.prepare_qat(net2) # insert observers\n",
        "net_quantized_qwt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JvHZzfEpkq2",
        "outputId": "aea3b668-ee75-4fb8-b1bf-c7d793f37ec7"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VerySimpleNet2(\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (linear1): Linear(\n",
              "    in_features=784, out_features=100, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (linear2): Linear(\n",
              "    in_features=100, out_features=100, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (linear3): Linear(\n",
              "    in_features=100, out_features=10, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (relu): ReLU()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model"
      ],
      "metadata": {
        "id": "6yVmTpVgqbqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_loader,net_quantized_qwt,epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJVVONsBqK6e",
        "outputId": "53c3a919-f937-43b7-bf17-e6aac3c5bbee"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 6000/6000 [00:56<00:00, 106.18it/s, loss=tensor(0.2204, grad_fn=<DivBackward0>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the statistics collected during training"
      ],
      "metadata": {
        "id": "A5uHGgVvrBYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Check statistics of the various layers')\n",
        "net_quantized_qwt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zunn-WoTqsUh",
        "outputId": "c49731a3-001d-4c84-bbdb-43e01549a3e8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check statistics of the various layers\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VerySimpleNet2(\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): MinMaxObserver(min_val=-0.4242129623889923, max_val=2.821486711502075)\n",
              "  )\n",
              "  (linear1): Linear(\n",
              "    in_features=784, out_features=100, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=-0.559889018535614, max_val=0.3510452210903168)\n",
              "    (activation_post_process): MinMaxObserver(min_val=-49.90431213378906, max_val=32.529666900634766)\n",
              "  )\n",
              "  (linear2): Linear(\n",
              "    in_features=100, out_features=100, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=-0.3996035158634186, max_val=0.4016411602497101)\n",
              "    (activation_post_process): MinMaxObserver(min_val=-28.026037216186523, max_val=19.46849822998047)\n",
              "  )\n",
              "  (linear3): Linear(\n",
              "    in_features=100, out_features=10, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=-0.5091035962104797, max_val=0.22812820971012115)\n",
              "    (activation_post_process): MinMaxObserver(min_val=-33.177005767822266, max_val=21.5955867767334)\n",
              "  )\n",
              "  (relu): ReLU()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantize the model using the statistics collected"
      ],
      "metadata": {
        "id": "KEEfVgicrave"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_quantized_qwt.eval()\n",
        "net_quantized_qwt = torch.ao.quantization.convert(net_quantized_qwt)"
      ],
      "metadata": {
        "id": "gpEh29yZrNMc"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Check statistics of the various layers')\n",
        "net_quantized_qwt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSz-xcQhrrgD",
        "outputId": "a6bf715c-bfd5-4f7e-b41c-54a72f8f46ed"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check statistics of the various layers\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VerySimpleNet2(\n",
              "  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)\n",
              "  (linear1): QuantizedLinear(in_features=784, out_features=100, scale=0.6490864157676697, zero_point=77, qscheme=torch.per_tensor_affine)\n",
              "  (linear2): QuantizedLinear(in_features=100, out_features=100, scale=0.37397274374961853, zero_point=75, qscheme=torch.per_tensor_affine)\n",
              "  (linear3): QuantizedLinear(in_features=100, out_features=10, scale=0.431280255317688, zero_point=77, qscheme=torch.per_tensor_affine)\n",
              "  (relu): ReLU()\n",
              "  (dequant): DeQuantize()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print weights and size of the model after quantization"
      ],
      "metadata": {
        "id": "w7u50gqYr7dJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Weights after quantization')\n",
        "print(torch.int_repr(net_quantized_qwt.linear1.weight()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwzEHsHfr1MG",
        "outputId": "e1a16785-e18e-470a-8855-2e4ebd2973cd"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights after quantization\n",
            "tensor([[-2, -4,  6,  ..., -3, -8,  0],\n",
            "        [ 1,  4,  1,  ..., -1,  0, -3],\n",
            "        [ 8, 10,  1,  ...,  5,  1,  8],\n",
            "        ...,\n",
            "        [-2,  6, -2,  ...,  6, -1,  3],\n",
            "        [10,  0, 10,  ...,  3,  9,  4],\n",
            "        [ 9,  9, -6,  ..., -5,  9, -5]], dtype=torch.int8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Testing the model after quantization')\n",
        "test(net_quantized_qwt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRj2_zhksWO9",
        "outputId": "91ac3ac1-2a97-4bb5-f10e-a6cd2171f7b5"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the model after quantization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 1000/1000 [00:03<00:00, 315.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "voQPwntbsdRl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}