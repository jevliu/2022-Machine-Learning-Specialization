{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMl3Y+WXyP9fZg8pusSJJ69",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jevliu/2022-Machine-Learning-Specialization/blob/main/ESB_OPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 加载必要的库"
      ],
      "metadata": {
        "id": "lKyRoJdltw8r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTAn7UzPtgUA"
      },
      "outputs": [],
      "source": [
        "from transformers import OPTForCausalLM, GPT2Tokenizer\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize_scalar\n",
        "import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 定义量化函数"
      ],
      "metadata": {
        "id": "UNsynqQkuPIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def esb_quantize(weights, b, k, alpha, eps=1e-8):\n",
        "    # 计算N\n",
        "    N = 2 ** (b - k - 1) - 1\n",
        "\n",
        "    # 生成移位因子xi\n",
        "    xi = [2 ** -k] + [2 ** (i - k - 1) for i in range(1, N + 1)]\n",
        "\n",
        "    # 生成分数集合Omega\n",
        "    Omega = [[0] + list(range(1, 2 ** k))] + [list(range(2 ** k, 2 ** (k + 1))) for _ in range(N)]\n",
        "\n",
        "    # 生成非负的量化值集合Q_positive\n",
        "    Q_positive = []\n",
        "    for i in range(N + 1):\n",
        "        Q_positive.extend([alpha * x * y for x in xi[i:] for y in Omega[i]])\n",
        "    Q_positive = sorted(set(Q_positive))\n",
        "\n",
        "    # 对量化值集合进行对称处理\n",
        "    Q_e = sorted(set([-q for q in Q_positive] + [0] + Q_positive))\n",
        "\n",
        "    # 计算量化范围C\n",
        "    C = np.max(Q_e)\n",
        "\n",
        "    # 创建掩码,标记非零权重\n",
        "    mask = weights != 0\n",
        "\n",
        "    # 缩放非零权重\n",
        "    scaled_weights = np.zeros_like(weights)\n",
        "    scaled_weights[mask] = weights[mask] / alpha\n",
        "\n",
        "    # 截断非零权重\n",
        "    clipped_weights = np.clip(scaled_weights, -C, C)\n",
        "\n",
        "    # 移位量化非零权重\n",
        "    quantized_weights = np.zeros_like(clipped_weights)\n",
        "    nonzero_indices = np.where(mask)\n",
        "    v = clipped_weights[nonzero_indices]\n",
        "    n = np.ceil(np.log2(np.abs(v) + eps))\n",
        "    quantized_v = np.round(v / (2 ** (n - k))) * (2 ** (n - k))\n",
        "    quantized_weights[nonzero_indices] = quantized_v\n",
        "\n",
        "    # 反缩放量化后的权重\n",
        "    dequantized_weights = quantized_weights * alpha\n",
        "\n",
        "    return dequantized_weights"
      ],
      "metadata": {
        "id": "DCFomJSSt1-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 定义加载模型和数据集的函数"
      ],
      "metadata": {
        "id": "-AVEu95JuUHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_tokenizer(model_name):\n",
        "    model = OPTForCausalLM.from_pretrained(model_name)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def text_dataset():\n",
        "    test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "    return test_dataset\n",
        "\n",
        "def encode_dataset(tokenizer, testdata):\n",
        "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
        "    return testenc\n"
      ],
      "metadata": {
        "id": "ab2ykVoBuY8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 定义量化加权量化误差函数和寻找最优的k和α的函数"
      ],
      "metadata": {
        "id": "5AbMI21pubm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quantization_error_weighted(weights, alpha, b, k, eps=1e-8):\n",
        "    quantized_weights = esb_quantize(weights, b, k, alpha)\n",
        "    abs_weights = np.abs(weights)\n",
        "    weights_sum = np.sum(abs_weights)\n",
        "    weighted_error = np.sum(np.abs(weights - quantized_weights) * abs_weights) / (weights_sum + eps)\n",
        "    return weighted_error\n",
        "\n",
        "def find_optimal_k_alpha(weights, b):\n",
        "    min_error = float(\"inf\")\n",
        "    optimal_k = None\n",
        "    optimal_alpha = None\n",
        "\n",
        "    for k in range(b):\n",
        "        alpha_opt = minimize_scalar(lambda a: quantization_error_weighted(weights, a, b, k)).x\n",
        "        error = quantization_error_weighted(weights, alpha_opt, b, k)\n",
        "        if error < min_error:\n",
        "            min_error = error\n",
        "            optimal_k = k\n",
        "            optimal_alpha = alpha_opt\n",
        "\n",
        "    return optimal_k, optimal_alpha, min_error\n",
        "\n",
        "def quantize_gpt2_layer(layer, b):\n",
        "    weights = layer.weight.cpu().detach().numpy()\n",
        "\n",
        "    optimal_k, optimal_alpha, min_error = find_optimal_k_alpha(weights, b)\n",
        "\n",
        "    quantized_weights = esb_quantize(weights, b, optimal_k, optimal_alpha)\n",
        "    layer.weight = torch.nn.Parameter(torch.tensor(quantized_weights))\n",
        "\n",
        "    print(f\"最优k: {optimal_k}, 最优α: {optimal_alpha:.4f}, 量化相对误差：{min_error:.8f}\")"
      ],
      "metadata": {
        "id": "MGUw-DJbufnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 针对模型进行量化的函数（只量化了Block）"
      ],
      "metadata": {
        "id": "uU2OmUKFvVCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize_opt_model(model, b):\n",
        "\n",
        "    start_time = time.time()\n",
        "    for i, layer in enumerate(model.model.decoder.layers):\n",
        "        print(f\"\\n对第{i+1}个Decoder Layer进行量化...\")\n",
        "\n",
        "        print(\"对注意力层进行量化...\")\n",
        "        quantize_gpt2_layer(layer.self_attn.k_proj, b)\n",
        "        quantize_gpt2_layer(layer.self_attn.v_proj, b)\n",
        "        quantize_gpt2_layer(layer.self_attn.q_proj, b)\n",
        "        quantize_gpt2_layer(layer.self_attn.out_proj, b)\n",
        "\n",
        "        print(\"对前馈神经网络层进行量化...\")\n",
        "        quantize_gpt2_layer(layer.fc1, b)\n",
        "        quantize_gpt2_layer(layer.fc2, b)\n",
        "\n",
        "        print(\"对Layer Norm层进行量化...\")\n",
        "        quantize_gpt2_layer(layer.self_attn_layer_norm, b)\n",
        "        quantize_gpt2_layer(layer.final_layer_norm, b)\n",
        "\n",
        "    # print(\"\\n对嵌入层进行量化...\")\n",
        "    # quantize_layer(copy_model.model.decoder.embed_tokens, b)\n",
        "    # quantize_layer(copy_model.model.decoder.embed_positions, b)\n",
        "\n",
        "    # print(\"\\n对最后的Layer Norm层进行量化...\")\n",
        "    # quantize_layer(copy_model.model.decoder.final_layer_norm, b)\n",
        "\n",
        "    # print(\"\\n对最后的线性层进行量化...\")\n",
        "    # quantize_layer(copy_model.lm_head, b)\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"量化总时间: {end_time - start_time:.2f} 秒\")"
      ],
      "metadata": {
        "id": "_dLUuKMXvLHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 使用GPTQ中给出的评估困惑度的函数"
      ],
      "metadata": {
        "id": "35AedEyZviZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "@torch.no_grad()\n",
        "# 评估OPT模型使用testenc数据和指定设备的性能\n",
        "def calculate_perplexity(model, testenc, dev):\n",
        "    '''\n",
        "    1.根据输入数据对OPT模型进行评估，评估性能指标，并计算模型的性能。\n",
        "    2.将指定的模型层移动到指定设备，并准备捕获输入数据和注意力掩码。\n",
        "    3.通过遍历模型的每个层，对每一层进行评估，根据评估结果计算性能指标，以便评估模型的性能表现。\n",
        "    '''\n",
        "    print('Evaluating ...')\n",
        "\n",
        "    testenc = testenc.input_ids\n",
        "    nsamples = testenc.numel() // 2048 # 根据testenc数据计算样本数\n",
        "\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.decoder.layers\n",
        "\n",
        "    # 将模型参数加载到指定设备\n",
        "    model.model.decoder.embed_tokens = model.model.decoder.embed_tokens.to(dev)\n",
        "    model.model.decoder.embed_positions = model.model.decoder.embed_positions.to(dev)\n",
        "    if hasattr(model.model.decoder, 'project_out') and model.model.decoder.project_out:\n",
        "        model.model.decoder.project_out = model.model.decoder.project_out.to(dev)\n",
        "    if hasattr(model.model.decoder, 'project_in') and model.model.decoder.project_in:\n",
        "        model.model.decoder.project_in = model.model.decoder.project_in.to(dev)\n",
        "    layers[0] = layers[0].to(dev)\n",
        "\n",
        "    dtype = next(iter(model.parameters())).dtype\n",
        "    inps = torch.zeros(\n",
        "        (nsamples, 2048, model.config.hidden_size), dtype=dtype, device=dev\n",
        "    )\n",
        "    cache = {'i': 0, 'attention_mask': None}\n",
        "\n",
        "    class Catcher(nn.Module):\n",
        "        def __init__(self, module):\n",
        "            super().__init__()\n",
        "            self.module = module\n",
        "        def forward(self, inp, **kwargs):\n",
        "            inps[cache['i']] = inp\n",
        "            cache['i'] += 1\n",
        "            cache['attention_mask'] = kwargs['attention_mask']\n",
        "            raise ValueError\n",
        "    layers[0] = Catcher(layers[0])\n",
        "    for i in range(nsamples):\n",
        "        batch = testenc[:, (i * 2048):((i + 1) * 2048)].to(dev)\n",
        "        try:\n",
        "            model(batch)\n",
        "        except ValueError:\n",
        "            pass\n",
        "    layers[0] = layers[0].module\n",
        "\n",
        "    layers[0] = layers[0].cpu()\n",
        "    model.model.decoder.embed_tokens = model.model.decoder.embed_tokens.cpu()\n",
        "    model.model.decoder.embed_positions = model.model.decoder.embed_positions.cpu()\n",
        "    if hasattr(model.model.decoder, 'project_out') and model.model.decoder.project_out:\n",
        "        model.model.decoder.project_out = model.model.decoder.project_out.cpu()\n",
        "    if hasattr(model.model.decoder, 'project_in') and model.model.decoder.project_in:\n",
        "        model.model.decoder.project_in = model.model.decoder.project_in.cpu()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    outs = torch.zeros_like(inps)\n",
        "    attention_mask = cache['attention_mask']\n",
        "\n",
        "    # 通过执行每个层并计算性能指标来进行评估\n",
        "    for i in range(len(layers)):\n",
        "        print(i)\n",
        "        # 对每一层进行评估，类似于opt_sequential函数\n",
        "        layer = layers[i].to(dev)\n",
        "        for j in range(nsamples):\n",
        "            outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
        "        layers[i] = layer.cpu()\n",
        "        del layer\n",
        "        torch.cuda.empty_cache()\n",
        "        inps, outs = outs, inps\n",
        "\n",
        "    if model.model.decoder.final_layer_norm is not None:\n",
        "        model.model.decoder.final_layer_norm = model.model.decoder.final_layer_norm.to(dev)\n",
        "    if model.model.decoder.project_out is not None:\n",
        "        model.model.decoder.project_out = model.model.decoder.project_out.to(dev)\n",
        "    model.lm_head = model.lm_head.to(dev)\n",
        "\n",
        "    testenc = testenc.to(dev)\n",
        "    nlls = []\n",
        "    for i in range(nsamples):\n",
        "        hidden_states = inps[i].unsqueeze(0)\n",
        "        if model.model.decoder.final_layer_norm is not None:\n",
        "            hidden_states = model.model.decoder.final_layer_norm(hidden_states)\n",
        "        if model.model.decoder.project_out is not None:\n",
        "            hidden_states = model.model.decoder.project_out(hidden_states)\n",
        "        lm_logits = model.lm_head(hidden_states)\n",
        "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
        "        shift_labels = testenc[\n",
        "            :, (i * 2048):((i + 1) * 2048)\n",
        "        ][:, 1:]\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "        neg_log_likelihood = loss.float() * 2048\n",
        "        nlls.append(neg_log_likelihood)\n",
        "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * 2048))\n",
        "    print(ppl.item())\n",
        "    return ppl.item()\n"
      ],
      "metadata": {
        "id": "N6J1XUH1vcOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 加载模型和测试数据"
      ],
      "metadata": {
        "id": "rO3WVtJRvsEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"facebook/opt-1.3b\"  # 可选: \"facebook/opt-125m\", \"facebook/opt-1.3b\", \"facebook/opt-2.7b\", \"facebook/opt-6.7b\", \"facebook/opt-13b\"\n",
        "\n",
        "model_13b, tokenizer = load_model_and_tokenizer(model_name)\n",
        "test_dataset = text_dataset()\n",
        "encodings = encode_dataset(tokenizer, test_dataset)"
      ],
      "metadata": {
        "id": "XqYAS1eSvrkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 指定量化位宽b并进行量化"
      ],
      "metadata": {
        "id": "wz_560gAvymq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "copy_model_4bit_13b = copy.deepcopy(model_13b)\n",
        "b = 4\n",
        "quantize_opt_model(copy_model_4bit_13b, b)"
      ],
      "metadata": {
        "id": "h5h_RSaMvyML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 评估量化后的困惑度"
      ],
      "metadata": {
        "id": "XFBz3l9wv56O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ppl = calculate_perplexity(copy_model_4bit_13b, encodings, dev)\n",
        "print(f\"{model_name}量化到{b}比特的困惑度(PPL): {ppl}\")"
      ],
      "metadata": {
        "id": "EfseHXwwv5Uz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}